# RAG Project: Retrieval-Augmented Generation Pipeline

## Project Description
This project implements a **Retrieval-Augmented Generation (RAG)** pipeline, designed to allow users to upload documents, query their content, and retrieve metadata. Built as part of an LLM Specialist Assignment, the system leverages modern AI and web technologies to provide an efficient and user-friendly experience for document-based question answering.

### Key Features
- **Document Ingestion**: Supports uploading up to 20 documents (PDFs or text), each with a maximum of 1000 pages. Documents are chunked into manageable sizes for efficient retrieval.
- **Vector-Based Retrieval**: Utilizes FAISS (a vector database) to store document embeddings generated by the SentenceTransformer model (`BAAI/bge-small-en-v1.5`), enabling fast and relevant retrieval of document chunks.
- **LLM-Powered Responses**: Integrates the Gemini API (`gemini-1.5-flash`) to generate accurate and concise responses based on retrieved document content.
- **REST API**: Built with FastAPI, providing three endpoints:
  - `POST /upload-documents`: Upload documents for processing.
  - `POST /query`: Query the system with questions (e.g., `{"query": "What is AI?"}`).
  - `GET /documents`: Retrieve metadata of uploaded documents (e.g., filename, page count, upload timestamp).
- **Containerization**: Includes a Docker setup with a `Dockerfile` and `docker-compose.yml` for easy local deployment and potential cloud deployment.
- **Testing**: Includes basic unit and integration tests (`tests/test_main.py`) for validating document upload, querying, and metadata retrieval.

### Technologies Used
- **Backend**: FastAPI (Python) for the REST API.
- **Vector Database**: FAISS for efficient similarity search of document embeddings.
- **Embedding Model**: SentenceTransformer (`BAAI/bge-small-en-v1.5`) for generating text embeddings.
- **LLM**: Gemini API (`gemini-1.5-flash`) for response generation.
- **Document Processing**: PyPDF2 for PDF parsing, LangChain for text chunking.
- **Containerization**: Docker and Docker Compose.
- **Testing**: Pytest for unit and integration tests.

### Purpose
The project demonstrates a practical implementation of a RAG system, combining document retrieval with LLM-based response generation. It showcases skills in API development, vector search, AI integration, and containerization, making it suitable for applications requiring intelligent document querying.

## Project Structure
```plaintext
RAG_Local/
├── README.md             # Project overview, setup, and usage instructions
├── requirements.txt      # Python package dependencies
├── .gitignore            # Files and directories to ignore in version control
├── src/                  # Source code directory
│   └── main.py           # Main script containing the RAG pipeline code
├── articles/             # Directory containing uploaded document files
│   ├── test.txt          # Example text file
│   └── sample.pdf        # Example PDF file
├── tests/                # Directory for unit and integration tests
│   └── test_main.py      # Test script for validating API endpoints
└── metadata.json         # Stores metadata of uploaded documents
